{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import subprocess\n",
    "import logging\n",
    "import os\n",
    "from loguru import logger\n",
    "import csv\n",
    "import urllib.parse\n",
    "from sentinelsat.sentinel import read_geojson, geojson_to_wkt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_credentials():\n",
    "    with open('credentials.json', 'r') as file:\n",
    "        credentials = json.load(file)\n",
    "    return credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadGranule_wget(options_and_url):\n",
    "    '''\n",
    "    This function is used to download a granule using the wget command-line utility. \n",
    "    It takes a single argument options_and_url which is a string that contains the options \n",
    "    and URL for the wget command. It returns the exit code of the wget command.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    options_and_url: A string that contains the options and URL for the wget command.\n",
    "    '''\n",
    "\n",
    "    cmd='wget -c --no-check-certificate -v ' + options_and_url\n",
    "    logging.debug(cmd)\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True)\n",
    "    return result.returncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadGranule(row, outdir):\n",
    "    '''\n",
    "    This function is used to download a granule from the Sentinel-1 SLC dataset. \n",
    "    It takes a single argument 'row', which is a dictionary containing the metadata \n",
    "    information about the granule to be downloaded. The function downloads the granule \n",
    "    from the specified download site(s) using the wget command-line utility, and saves \n",
    "    the zip file to a directory on the local file system.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row: A dictionary containing metadata information about the granule to be downloaded. \n",
    "         The dictionary should contain the following keys:\n",
    "         - 'Download Site': A string indicating the download site ('AWS' or 'ASF' or 'both').\n",
    "         - 'Path Number': A string representing the path number of the granule.\n",
    "         - 'Frame Number': A string representing the frame number of the granule.\n",
    "         - 'Granule Name': A string representing the name of the granule.\n",
    "         - 'Acquisition Date': A string representing the date of the granule acquisition.\n",
    "         - 'asf_wget_str': A string containing additional options for the wget command when downloading \n",
    "                           from the ASF download site (if applicable).\n",
    "         - 'URL': A string representing the URL for downloading from the ASF download site (if applicable).\n",
    "    \n",
    "    '''\n",
    "\n",
    "    aws_baseurl = 'http://sentinel1-slc-seasia-pds.s3-website-ap-southeast-1.amazonaws.com/datasets/slc/v1.1/'\n",
    "    download_site = row['Download Site']\n",
    "    frame_dir='P' + row['Path Number'].zfill(3) + '/F' + row['Frame Number'].zfill(4)\n",
    "    frame_dir = os.path.join(outdir,frame_dir)\n",
    "    logger.info(f\"Downloading granule {row['Granule Name']} to directory {frame_dir}.\")\n",
    "    output = os.makedirs(frame_dir, exist_ok=True)\n",
    "    os.chdir(frame_dir)\n",
    "\n",
    "    urls = []\n",
    "    if download_site in {'AWS', 'both'}:\n",
    "        row_date = row['Acquisition Date']\n",
    "        date_folder = f\"{row_date[:4]}/{row_date[5:7]}/{row_date[8:10]}/\"\n",
    "        aws_url = f\"{aws_baseurl}{date_folder}{row['Granule Name']}/{row['Granule Name']}.zip\"\n",
    "        urls.append(aws_url)\n",
    "\n",
    "    if download_site in {'ASF', 'both'}:\n",
    "        urls.append(f\"{row['asf_wget_str']} {row['URL']}\")\n",
    "\n",
    "    for url in urls:\n",
    "        status = downloadGranule_wget(url)\n",
    "        if status == 0:\n",
    "            logger.info(f\"{url} download succeeded.\")\n",
    "            break\n",
    "        else:\n",
    "            logger.info(f\"{url} download failed.\")\n",
    "\n",
    "    os.chdir(outdir)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_sentinel_1(startdate, enddate, geom):\n",
    "    ''' \n",
    "    This function searches for Sentinel-1 satellite images in the Alaska \n",
    "    Satellite Facility (ASF) archive within a specific time range and geographic boundary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    startdate: A string in the format YYYY-MM-DD, representing the start date of the time range for the search.\n",
    "    enddate: A string in the format YYYY-MM-DD, representing the end date of the time range for the search.\n",
    "    geom: A geometry object representing the geographic boundary of the search area.\n",
    "    '''\n",
    "\n",
    "    output_format = 'csv'\n",
    "    # hard-coded ASF query URL:\n",
    "    asf_baseurl='https://api.daac.asf.alaska.edu/services/search/param?'\n",
    "    # Use a dictionary to store the query parameters\n",
    "    # Read GeoJSON file into a dictionary\n",
    "    geojson_dict = read_geojson(geom)\n",
    "\n",
    "    # Convert to WKT format\n",
    "    wkt = geojson_to_wkt(geojson_dict['features'][0]['geometry'])\n",
    "\n",
    "    query_params = {\n",
    "        'output': 'csv',\n",
    "        'platform': 'Sentinel-1A, Sentinel-1B',\n",
    "        'processingLevel': 'GRD_HD',\n",
    "        'beamMode': 'IW',\n",
    "        'intersectsWith': wkt, \n",
    "        'start': f'{startdate}T00:00:00UTC',\n",
    "        'end': f'{enddate}T00:00:00UTC'\n",
    "    }\n",
    "\n",
    "    # Use urllib.parse.urlencode() to encode the query parameters\n",
    "    encoded_params = urllib.parse.urlencode(query_params)\n",
    "\n",
    "    # Combine the encoded query parameters with the ASF base URL to form the complete URL\n",
    "    query_url = asf_baseurl + encoded_params\n",
    "\n",
    "    # Make the request to the ASF API\n",
    "    logger.info('\\nRunning ASF API query:')\n",
    "    logger.info(query_url + '\\n')\n",
    "    response = requests.post(query_url)\n",
    "\n",
    "    # Parse the response if it's in CSV format\n",
    "    if output_format == 'csv':\n",
    "        # Use csv.DictReader to parse the CSV response into a list of dictionaries\n",
    "        reader = csv.DictReader(response.text.splitlines())\n",
    "        rows = list(reader)\n",
    "\n",
    "        # Log the number of scenes found and their details\n",
    "        num_scenes = len(rows)\n",
    "        if num_scenes > 0:\n",
    "            logger.info(f\"Found {num_scenes} scene{'s' if num_scenes > 1 else ''}.\")\n",
    "            for row in rows:\n",
    "                logger.info(f\"Scene {row['Granule Name']}, Path {row['Path Number']} / Frame {row['Frame Number']}\")\n",
    "    \n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_download_info(rows, download_site):\n",
    "    '''\n",
    "    This function takes a list of dictionaries containing search results for Sentinel-1 scenes and adds additional download \n",
    "    information to each dictionary. Specifically, it adds the 'Download Site' key to each dictionary with the value specified in \n",
    "    the global variable 'download_site', and adds an 'asf_wget_str' key to each dictionary if the download site is not AWS. \n",
    "    The 'asf_wget_str' key contains options that need to be passed to the wget command for ASF downloads (http-user and http-password).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rows: A list of dictionaries, where each dictionary contains metadata about a Sentinel-1 scene.\n",
    "    \n",
    "    Return Value:\n",
    "    ----------\n",
    "    This function returns the modified list of dictionaries with additional download information added to each dictionary.\n",
    "    '''\n",
    "    credentials = load_credentials()\n",
    "    for row in rows:\n",
    "        if download_site != 'AWS':\n",
    "            # Pass http-user and http-password for ASF downloads\n",
    "            asf_wget_options = {\n",
    "                'http-user':credentials['user'],\n",
    "                'http-password':credentials['password']} #Ask me for the password\n",
    "            row['asf_wget_str'] = ' '.join('--%s=%s'%(item[0],item[1]) for item in asf_wget_options.items())\n",
    "        else:\n",
    "            row['asf_wget_str'] = ''\n",
    "        row['Download Site'] = download_site\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_sentinel_1(nproc, rows, outdir):\n",
    "    '''\n",
    "    This function downloads Sentinel-1 data in parallel. It takes as input the number of parallel\n",
    "    download processes to run and a list of dictionaries containing information about each granule to download.\n",
    "    The function uses the 'wget' command to download the data. If the 'download_site' parameter is set to 'AWS',\n",
    "    the data will be downloaded from the AWS S3 bucket. If it is set to 'ASF', the data will be downloaded from\n",
    "    the ASF DAAC. If it is set to 'both', the function will attempt to download the data from both sites.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    nproc : int\n",
    "        The number of parallel download processes to run.\n",
    "    rows : list of dict\n",
    "        A list of dictionaries containing information about each granule to download.\n",
    "        Each dictionary should have the following keys: 'Granule Name', 'Download URL', 'asf_wget_str', 'Download Site'.\n",
    "    '''\n",
    "    \n",
    "    logger.info(f\"\\nRunning {nproc} downloads in parallel.\")\n",
    "    download_site = 'both'\n",
    "    downloadList = add_download_info(rows, download_site)\n",
    "\n",
    "    for product in downloadList:\n",
    "        logger.info(\"Attempting to download\")\n",
    "        downloadGranule(product, outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-11 15:47:55.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msearch_sentinel_1\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1m\n",
      "Running ASF API query:\u001b[0m\n",
      "\u001b[32m2023-12-11 15:47:55.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msearch_sentinel_1\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mhttps://api.daac.asf.alaska.edu/services/search/param?output=csv&platform=Sentinel-1A%2C+Sentinel-1B&processingLevel=GRD_HD&beamMode=IW&intersectsWith=MULTIPOLYGON%28%28%2820.4338+39.3408%2C19.3671+39.9610%2C23.2619+42.0945%2C27.7522+41.8464%2C28.5460+37.5547%2C28.5460+34.3792%2C25.7427+33.6846%2C21.6742+34.7266%2C20.0617+36.7608%2C20.4338+39.3408%29%29%29&start=2022-04-01T00%3A00%3A00UTC&end=2022-04-05T00%3A00%3A00UTC\n",
      "\u001b[0m\n",
      "\u001b[32m2023-12-11 15:47:58.258\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msearch_sentinel_1\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mFound 18 scenes.\u001b[0m\n",
      "\u001b[32m2023-12-11 15:47:58.259\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msearch_sentinel_1\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mScene S1A_IW_GRDH_1SDV_20220404T161636_20220404T161701_042626_0515EB_8A42, Path 29 / Frame 133\u001b[0m\n",
      "\u001b[32m2023-12-11 15:47:58.260\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msearch_sentinel_1\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mScene S1A_IW_GRDH_1SDV_20220404T161611_20220404T161636_042626_0515EB_08CB, Path 29 / Frame 128\u001b[0m\n",
      "\u001b[32m2023-12-11 15:47:58.261\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msearch_sentinel_1\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mScene S1A_IW_GRDH_1SDV_20220404T161546_20220404T161611_042626_0515EB_E95A, Path 29 / Frame 123\u001b[0m\n",
      "\u001b[32m2023-12-11 15:47:58.262\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msearch_sentinel_1\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mScene S1A_IW_GRDH_1SDV_20220404T161521_20220404T161546_042626_0515EB_7021, Path 29 / Frame 118\u001b[0m\n",
      "\u001b[32m2023-12-11 15:47:58.263\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msearch_sentinel_1\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mScene S1A_IW_GRDH_1SDV_20220404T161456_20220404T161521_042626_0515EB_436C, Path 29 / Frame 113\u001b[0m\n",
      "\u001b[32m2023-12-11 15:47:58.264\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msearch_sentinel_1\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mScene S1A_IW_GRDH_1SDV_20220404T161431_20220404T161456_042626_0515EB_34FE, Path 29 / Frame 108\u001b[0m\n",
      "\u001b[32m2023-12-11 15:47:58.265\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msearch_sentinel_1\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mScene S1A_IW_GRDH_1SDV_20220403T043258_20220403T043323_042604_051520_D54C, Path 7 / Frame 478\u001b[0m\n",
      "\u001b[32m2023-12-11 15:47:58.266\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msearch_sentinel_1\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mScene S1A_IW_GRDH_1SDV_20220403T043233_20220403T043258_042604_051520_1894, Path 7 / Frame 473\u001b[0m\n",
      "\u001b[32m2023-12-11 15:47:58.267\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msearch_sentinel_1\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mScene S1A_IW_GRDH_1SDV_20220403T043208_20220403T043233_042604_051520_0FE1, Path 7 / Frame 468\u001b[0m\n",
      "\u001b[32m2023-12-11 15:47:58.267\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msearch_sentinel_1\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mScene S1A_IW_GRDH_1SDV_20220403T043143_20220403T043208_042604_051520_BA3E, Path 7 / Frame 463\u001b[0m\n",
      "\u001b[32m2023-12-11 15:47:58.268\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msearch_sentinel_1\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mScene S1A_IW_GRDH_1SDV_20220403T043118_20220403T043143_042604_051520_3C00, Path 7 / Frame 458\u001b[0m\n",
      "\u001b[32m2023-12-11 15:47:58.268\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msearch_sentinel_1\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mScene S1A_IW_GRDH_1SDV_20220403T043053_20220403T043118_042604_051520_1143, Path 7 / Frame 453\u001b[0m\n",
      "\u001b[32m2023-12-11 15:47:58.268\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msearch_sentinel_1\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mScene S1A_IW_GRDH_1SDV_20220402T163259_20220402T163324_042597_0514E1_CE7A, Path 175 / Frame 132\u001b[0m\n",
      "\u001b[32m2023-12-11 15:47:58.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msearch_sentinel_1\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mScene S1A_IW_GRDH_1SDV_20220402T163234_20220402T163259_042597_0514E1_D0AA, Path 175 / Frame 127\u001b[0m\n",
      "\u001b[32m2023-12-11 15:47:58.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msearch_sentinel_1\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mScene S1A_IW_GRDH_1SDV_20220402T163209_20220402T163234_042597_0514E1_07F2, Path 175 / Frame 122\u001b[0m\n",
      "\u001b[32m2023-12-11 15:47:58.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msearch_sentinel_1\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mScene S1A_IW_GRDH_1SDV_20220402T163144_20220402T163209_042597_0514E1_2E52, Path 175 / Frame 117\u001b[0m\n",
      "\u001b[32m2023-12-11 15:47:58.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msearch_sentinel_1\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mScene S1A_IW_GRDH_1SDV_20220402T163119_20220402T163144_042597_0514E1_0F02, Path 175 / Frame 112\u001b[0m\n",
      "\u001b[32m2023-12-11 15:47:58.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msearch_sentinel_1\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mScene S1A_IW_GRDH_1SDV_20220402T163054_20220402T163119_042597_0514E1_7A13, Path 175 / Frame 107\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Set your area of interest (polygon projected in 4326)\n",
    "aoi_geojson = \".../polygon_example.geojson\" #Path to AOI\n",
    " \n",
    "#Specify starting date\n",
    "startdate = \"2022-04-01\"\n",
    "#Specify ending date\n",
    "enddate = \"2022-04-05\"\n",
    "\n",
    "#How many images to be downloaded in parallel\n",
    "nproc = 20\n",
    "#Set to TRUE if you want to download the found S1 products\n",
    "download = False\n",
    "\n",
    "#Specify output directory to save the downloaded S1 products\n",
    "outdir = \".../path-to-output\" #Path to output directory\n",
    "\n",
    "rows = search_sentinel_1(startdate, enddate, aoi_geojson)\n",
    "if download:\n",
    "    download_sentinel_1(nproc, rows, outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rs-download",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
